{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from anthropic import Anthropic\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import openai\n",
    "import numpy as np\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gpt-3.5 turbo / scores only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run chatgpt_parallel.py --prompt_fp prompts/summeval/coh_detailed.txt --save_fp results/gpt35_coh.json --max_tokens 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run chatgpt_parallel.py --prompt_fp prompts/summeval/con_detailed.txt --save_fp results/gpt35_con.json --max_tokens 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run chatgpt_parallel.py --prompt_fp prompts/summeval/flu_detailed.txt --save_fp results/gpt35_flu.json --max_tokens 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run chatgpt_parallel.py --prompt_fp prompts/summeval/rel_detailed.txt --save_fp results/gpt35_rel.json --max_tokens 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gpt-3.5 turbo / scores with justification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run chatgpt_parallel.py --prompt_fp prompts/summeval/coh_justified.txt --save_fp results/gpt35_coh_just.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run chatgpt_parallel.py --prompt_fp prompts/summeval/con_justified.txt --save_fp results/gpt35_con_just.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run chatgpt_parallel.py --prompt_fp prompts/summeval/flu_justified.txt --save_fp results/gpt35_flu_just.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run chatgpt_parallel.py --prompt_fp prompts/summeval/rel_justified.txt --save_fp results/gpt35_rel_just.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### claude haiku / scores only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run claude_parallel.py --prompt_fp prompts/summeval/coh_detailed.txt --save_fp results/haiku_coh.json --max_tokens 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run claude_parallel.py --prompt_fp prompts/summeval/con_detailed.txt --save_fp results/haiku_con.json --max_tokens 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run claude_parallel.py --prompt_fp prompts/summeval/flu_detailed.txt --save_fp results/haiku_flu.json --max_tokens 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run claude_parallel.py --prompt_fp prompts/summeval/rel_detailed.txt --save_fp results/haiku_rel.json --max_tokens 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### claude haiku / scores with justification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run claude_parallel.py --prompt_fp prompts/summeval/coh_justified.txt --save_fp results/haiku_coh_just.json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run claude_parallel.py --prompt_fp prompts/summeval/con_justified.txt --save_fp results/haiku_con_just.json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run claude_parallel.py --prompt_fp prompts/summeval/flu_justified.txt --save_fp results/haiku_flu_just.json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run claude_parallel.py --prompt_fp prompts/summeval/rel_justified.txt --save_fp results/haiku_rel_just.json "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gpt-4 turbo / scores only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run chatgpt_parallel.py --prompt_fp prompts/summeval/coh_detailed.txt --save_fp results/gpt4_coh.json --max_tokens 6 --model 'gpt-4-turbo-2024-04-09'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600/1600 [09:29<00:00,  2.81it/s]\n"
     ]
    }
   ],
   "source": [
    "%run chatgpt_parallel.py --prompt_fp prompts/summeval/flu_detailed.txt --save_fp results/gpt4_flu.json --max_tokens 6 --model 'gpt-4-turbo-2024-04-09'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600/1600 [13:30<00:00,  1.97it/s] \n"
     ]
    }
   ],
   "source": [
    "%run chatgpt_parallel.py --prompt_fp prompts/summeval/con_detailed.txt --save_fp results/gpt4_con.json --max_tokens 6 --model 'gpt-4-turbo-2024-04-09'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600/1600 [09:31<00:00,  2.80it/s]\n"
     ]
    }
   ],
   "source": [
    "%run chatgpt_parallel.py --prompt_fp prompts/summeval/rel_detailed.txt --save_fp results/gpt4_rel.json --max_tokens 6 --model 'gpt-4-turbo-2024-04-09'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### claude sonnet / scores only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600/1600 [1:00:21<00:00,  2.26s/it]\n"
     ]
    }
   ],
   "source": [
    "%run claude_parallel.py --prompt_fp prompts/summeval/coh_detailed.txt --save_fp results/sonnet_coh.json --max_tokens 6 --model 'claude-3-sonnet-20240229' --n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600/1600 [2:04:57<00:00,  4.69s/it]    \n"
     ]
    }
   ],
   "source": [
    "%run claude_parallel.py --prompt_fp prompts/summeval/flu_detailed.txt --save_fp results/sonnet_flu.json --max_tokens 6 --model 'claude-3-sonnet-20240229' --n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600/1600 [58:25<00:00,  2.19s/it] \n"
     ]
    }
   ],
   "source": [
    "%run claude_parallel.py --prompt_fp prompts/summeval/con_detailed.txt --save_fp results/sonnet_con.json --max_tokens 6 --model 'claude-3-sonnet-20240229' --n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600/1600 [1:00:25<00:00,  2.27s/it]\n"
     ]
    }
   ],
   "source": [
    "%run claude_parallel.py --prompt_fp prompts/summeval/rel_detailed.txt --save_fp results/sonnet_rel.json --max_tokens 6 --model 'claude-3-sonnet-20240229' --n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### claude opus / scores only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600/1600 [10:45<00:00,  2.48it/s]\n"
     ]
    }
   ],
   "source": [
    "%run claude_parallel.py --prompt_fp prompts/summeval/coh_detailed.txt --save_fp results/opus_coh.json --max_tokens 6 --model 'claude-3-opus-20240229' --n 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600/1600 [1:19:20<00:00,  2.98s/it]     \n"
     ]
    }
   ],
   "source": [
    "%run claude_parallel.py --prompt_fp prompts/summeval/flu_detailed.txt --save_fp results/opus_flu.json --max_tokens 6 --model 'claude-3-opus-20240229' --n 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600/1600 [10:48<00:00,  2.47it/s]\n"
     ]
    }
   ],
   "source": [
    "%run claude_parallel.py --prompt_fp prompts/summeval/con_detailed.txt --save_fp results/opus_con.json --max_tokens 6 --model 'claude-3-opus-20240229' --n 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600/1600 [10:50<00:00,  2.46it/s]\n"
     ]
    }
   ],
   "source": [
    "%run claude_parallel.py --prompt_fp prompts/summeval/rel_detailed.txt --save_fp results/opus_rel.json --max_tokens 6 --model 'claude-3-opus-20240229' --n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gpt-4o / scores only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600/1600 [04:10<00:00,  6.39it/s]\n"
     ]
    }
   ],
   "source": [
    "%run chatgpt_parallel.py --prompt_fp prompts/summeval/coh_detailed.txt --save_fp results/gpt4o_coh.json --max_tokens 6 --model 'gpt-4o-2024-05-13'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600/1600 [04:07<00:00,  6.47it/s]\n"
     ]
    }
   ],
   "source": [
    "%run chatgpt_parallel.py --prompt_fp prompts/summeval/flu_detailed.txt --save_fp results/gpt4o_flu.json --max_tokens 6 --model 'gpt-4o-2024-05-13'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600/1600 [04:01<00:00,  6.63it/s]\n"
     ]
    }
   ],
   "source": [
    "%run chatgpt_parallel.py --prompt_fp prompts/summeval/con_detailed.txt --save_fp results/gpt4o_con.json --max_tokens 6 --model 'gpt-4o-2024-05-13'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600/1600 [04:04<00:00,  6.54it/s]\n"
     ]
    }
   ],
   "source": [
    "%run chatgpt_parallel.py --prompt_fp prompts/summeval/rel_detailed.txt --save_fp results/gpt4o_rel.json --max_tokens 6 --model 'gpt-4o-2024-05-13'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fluency prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/guillaumedherouville/Library/CloudStorage/OneDrive-HECParis/US/Summary scoring/geval - local'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [02:06<00:00,  6.31it/s]\n"
     ]
    }
   ],
   "source": [
    "%run chatgpt_parallel.py --prompt_fp prompts/summeval/flu_test_1.txt --save_fp results/prompting/gpt4o_flu_1.json --max_tokens 6 --model 'gpt-4o-2024-05-13'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_movie_token_test(prompt, data, model):\n",
    "    openai.api_key = \"\"\n",
    "    movie = json.load(open(data))\n",
    "    prompt = open(prompt).read()\n",
    "    new_json = []\n",
    "    for instance in movie:\n",
    "        source = instance[\"source\"]\n",
    "        system_output = instance[\"system_output\"]\n",
    "        cur_prompt = prompt.replace(\"{{Document}}\", source).replace(\n",
    "            \"{{Summary}}\", system_output\n",
    "        )\n",
    "        instance[\"prompt\"] = cur_prompt\n",
    "        while True:\n",
    "            _response = openai.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": cur_prompt}],\n",
    "                temperature=1,\n",
    "                max_tokens=6,\n",
    "                top_p=1,\n",
    "                frequency_penalty=0,\n",
    "                presence_penalty=0,\n",
    "                stop=None,\n",
    "            )\n",
    "            time.sleep(0.5)\n",
    "            all_responses = [\n",
    "                _choice.message.content for _choice in _response.choices\n",
    "            ]\n",
    "            instance[\"all_responses\"] = all_responses\n",
    "            new_json.append(instance)\n",
    "            break\n",
    "\n",
    "    tokenizer = tiktoken.encoding_for_model(\"gpt-4o-2024-05-13\")\n",
    "    tokens = tokenizer.encode(cur_prompt)\n",
    "    print(f\"{len(tokens) + 7} prompt tokens counted by num_tokens_from_messages().\")\n",
    "    print(f'{_response.usage.prompt_tokens} prompt tokens counted by the OpenAI API.')\n",
    "\n",
    "    return new_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/guillaumedherouville/Library/CloudStorage/OneDrive-HECParis/US/Summary scoring/geval - local'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24201 prompt tokens counted by num_tokens_from_messages().\n",
      "24201 prompt tokens counted by the OpenAI API.\n"
     ]
    }
   ],
   "source": [
    "result = gpt_movie_token_test(\"prompts/summeval/coh_movie_grained.txt\", \"data/logline/dunkirk_log.json\", \"gpt-4o-2024-05-13\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MOVIE SCRIPTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def claude_movie_analysis(prompt, data, model, n, save):\n",
    "\n",
    "    client = Anthropic(\n",
    "    api_key=\"\"\n",
    ")\n",
    "\n",
    "    summeval = json.load(open(data))\n",
    "    prompt = open(prompt).read()\n",
    "\n",
    "    new_json = []\n",
    "    for instance in summeval:\n",
    "        source = instance[\"source\"]\n",
    "        system_output = instance[\"system_output\"]\n",
    "        cur_prompt = prompt.replace(\"{{Document}}\", source).replace(\n",
    "            \"{{Summary}}\", system_output\n",
    "        )\n",
    "        instance[\"prompt\"] = cur_prompt\n",
    "        attempt = 0\n",
    "        while True:\n",
    "            try:\n",
    "                all_responses = []\n",
    "                for i in tqdm.tqdm(range(n), leave=False):\n",
    "                    _response = client.messages.create(\n",
    "                        model=model,\n",
    "                        messages=[{\"role\": \"user\", \"content\": cur_prompt}],\n",
    "                        temperature=1,\n",
    "                        max_tokens=6,\n",
    "                    )\n",
    "                    time.sleep(0.5)\n",
    "                    response = _response.content[0].text\n",
    "                    all_responses.append(response)\n",
    "                instance[\"all_responses\"] = all_responses\n",
    "                new_json.append(instance)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                if \"RateLimitError\" in str(e) or \"rate_limit_error\" in str(e).lower():\n",
    "                        attempt += 1\n",
    "                        sleep_time = min(60, (2 ** attempt) + random.uniform(0, 1))  # Exponential backoff with a cap at 60 seconds\n",
    "                        print(f\"Rate limit hit, sleeping for {sleep_time:.2f} seconds\")\n",
    "                        time.sleep(sleep_time)\n",
    "                else:\n",
    "                    print(f\"Unhandled exception: {e}\")\n",
    "                    break\n",
    "    \n",
    "    with open(save, \"w\") as f:\n",
    "        json.dump(new_json, f, indent=4)\n",
    "\n",
    "    return new_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unhandled exception: Error code: 401 - {'type': 'error', 'error': {'type': 'authentication_error', 'message': 'invalid x-api-key'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'all_responses'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m df_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(filename)[\u001b[38;5;241m0\u001b[39m] \n\u001b[1;32m      7\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(result)\n\u001b[0;32m----> 8\u001b[0m df\u001b[38;5;241m.\u001b[39mloc[:, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprob_score\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mall_responses\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39mmean([\u001b[38;5;28mint\u001b[39m(i[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m x \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01mand\u001b[39;00m i[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39misdigit()]))\n\u001b[1;32m     10\u001b[0m df \u001b[38;5;241m=\u001b[39m [df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall_responses\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m], df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprob_score\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mglobals\u001b[39m()[df_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_claude_coh\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/LLM_research/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/LLM_research/lib/python3.12/site-packages/pandas/core/indexes/range.py:417\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[0;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'all_responses'"
     ]
    }
   ],
   "source": [
    "directory = 'data'\n",
    "claude_coh_dict = {}\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".json\"):\n",
    "        result = claude_movie_analysis(\"prompts/summeval/coh_movie.txt\", os.path.join(directory, filename), \"claude-3-opus-20240229\", n=1, save='results/movies/claude/' + filename)\n",
    "        df_name = os.path.splitext(filename)[0] \n",
    "        df = pd.DataFrame(result)\n",
    "        df.loc[:, 'prob_score'] = df['all_responses'].apply(\n",
    "            lambda x: np.mean([int(i[-1]) for i in x if i and i[-1].isdigit()]))\n",
    "        df = [df['all_responses'][0], df['prob_score'][0]]\n",
    "        globals()[df_name + '_claude_coh'] = df\n",
    "        claude_coh_dict[df_name + '_claude_coh'] = df\n",
    "    else: \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_movie_analysis(prompt, data, model, n, save):\n",
    "    openai.api_key = \"\"\n",
    "    summeval = json.load(open(data))\n",
    "    prompt = open(prompt).read()\n",
    "    new_json = []\n",
    "    for instance in summeval:\n",
    "        source = instance[\"source\"]\n",
    "        system_output = instance[\"system_output\"]\n",
    "        cur_prompt = prompt.replace(\"{{Document}}\", source).replace(\n",
    "            \"{{Summary}}\", system_output\n",
    "        )\n",
    "        instance[\"prompt\"] = cur_prompt\n",
    "        while True:\n",
    "            _response = openai.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": cur_prompt}],\n",
    "                temperature=1,\n",
    "                max_tokens=6,\n",
    "                top_p=1,\n",
    "                frequency_penalty=0,\n",
    "                presence_penalty=0,\n",
    "                stop=None,\n",
    "                n=n,\n",
    "            )\n",
    "            time.sleep(0.5)\n",
    "            all_responses = [\n",
    "                _choice.message.content for _choice in _response.choices\n",
    "            ]\n",
    "            instance[\"all_responses\"] = all_responses\n",
    "            new_json.append(instance)\n",
    "            break\n",
    "        \n",
    "    with open(save, \"w\") as f:\n",
    "        json.dump(new_json, f, indent=4)\n",
    "\n",
    "    return new_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### COHERENCE\n",
    "\n",
    "directory = 'data'\n",
    "gpt_coh_grained = {}\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".json\"):\n",
    "        result = gpt_movie_analysis(\"prompts/summeval/coh_movie_grained.txt\", os.path.join(directory, filename), \"gpt-4-turbo-2024-04-09\", n=10, save='results/movies/gpt/' + filename)\n",
    "        df_name = os.path.splitext(filename)[0] \n",
    "        df = pd.DataFrame(result)\n",
    "        df.loc[:, 'prob_score'] = df['all_responses'].apply(\n",
    "            lambda x: np.mean([int(i[-1]) for i in x if i and i[-1].isdigit()]))\n",
    "        df = [df['all_responses'][0], df['prob_score'][0]]\n",
    "        globals()[df_name + '_gpt_coh_grained'] = df\n",
    "        gpt_coh_grained_dict[df_name + '_gpt_coh_grained'] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_out_2_gpt_coh_grained : 4.2\n",
      "frat_boy_genius_gpt_coh_grained : 3.2\n",
      "king_richard_gpt_coh_grained : 4.9\n",
      "frat_boy_genius_2_gpt_coh_grained : 4.1\n",
      "inception_gpt_coh_grained : 3.8\n",
      "dunkirk_2_gpt_coh_grained : 4.8\n",
      "dunkirk_gpt_coh_grained : 4.6\n",
      "king_richard_2_gpt_coh_grained : 4.8\n",
      "get_out_gpt_coh_grained : 4.6\n",
      "inception_2_gpt_coh_grained : 4.7\n"
     ]
    }
   ],
   "source": [
    "for name, value in gpt_coh_grained.items():\n",
    "    print(f\"{name} : {value[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RELEVANCE\n",
    "\n",
    "directory = 'data'\n",
    "gpt_rel_grained_dict = {}\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".json\"):\n",
    "        result = gpt_movie_analysis(\"prompts/summeval/rel_movie_grained.txt\", os.path.join(directory, filename), \"gpt-4-turbo-2024-04-09\", n=10, save='results/movies/gpt/' + os.path.splitext(filename)[0] + '_rel_grained.json')\n",
    "        df_name = os.path.splitext(filename)[0] \n",
    "        df = pd.DataFrame(result)\n",
    "        df.loc[:, 'prob_score'] = df['all_responses'].apply(\n",
    "            lambda x: np.mean([int(i[-1]) for i in x if i and i[-1].isdigit()]))\n",
    "        df = [df['all_responses'][0], df['prob_score'][0]]\n",
    "        globals()[df_name + '_gpt_rel_grained'] = df\n",
    "        gpt_rel_grained_dict[df_name + '_gpt_rel_grained'] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_out_2_gpt_rel_grained : 4.0\n",
      "frat_boy_genius_gpt_rel_grained : 4.1\n",
      "king_richard_gpt_rel_grained : 4.9\n",
      "frat_boy_genius_2_gpt_rel_grained : 4.2\n",
      "inception_gpt_rel_grained : 4.9\n",
      "dunkirk_2_gpt_rel_grained : 3.8\n",
      "dunkirk_gpt_rel_grained : 4.9\n",
      "king_richard_2_gpt_rel_grained : 4.8\n",
      "get_out_gpt_rel_grained : 4.4\n",
      "inception_2_gpt_rel_grained : 4.3\n"
     ]
    }
   ],
   "source": [
    "for name, value in gpt_rel_grained.items():\n",
    "    print(f\"{name} : {value[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSISTENCY\n",
    "\n",
    "directory = 'data'\n",
    "gpt_con_grained = {}\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".json\"):\n",
    "        result = gpt_movie_analysis(\"prompts/summeval/con_movie_grained.txt\", os.path.join(directory, filename), \"gpt-4-turbo-2024-04-09\", n=10, save='results/movies/gpt/' + os.path.splitext(filename)[0] + '_con_grained.json')\n",
    "        df_name = os.path.splitext(filename)[0] \n",
    "        df = pd.DataFrame(result)\n",
    "        df.loc[:, 'prob_score'] = df['all_responses'].apply(\n",
    "            lambda x: np.mean([int(i[-1]) for i in x if i and i[-1].isdigit()]))\n",
    "        df = [df['all_responses'][0], df['prob_score'][0]]\n",
    "        globals()[df_name + '_gpt_con_grained'] = df\n",
    "        gpt_con_grained[df_name + '_gpt_con_grained'] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_out_2_gpt_con_grained : 4.9\n",
      "frat_boy_genius_gpt_con_grained : 4.2\n",
      "king_richard_gpt_con_grained : 4.7\n",
      "frat_boy_genius_2_gpt_con_grained : 3.0\n",
      "inception_gpt_con_grained : 2.4\n",
      "dunkirk_2_gpt_con_grained : 1.6\n",
      "dunkirk_gpt_con_grained : 2.3\n",
      "king_richard_2_gpt_con_grained : 4.6\n",
      "get_out_gpt_con_grained : 2.4\n",
      "inception_2_gpt_con_grained : 4.6\n"
     ]
    }
   ],
   "source": [
    "for name, value in gpt_con_grained.items():\n",
    "    print(f\"{name} : {value[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FLUENCY\n",
    "\n",
    "directory = 'data'\n",
    "gpt_flu_grained = {}\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".json\"):\n",
    "        result = gpt_movie_analysis(\"prompts/summeval/flu_movie_grained.txt\", os.path.join(directory, filename), \"gpt-4-turbo-2024-04-09\", n=10, save='results/movies/gpt/' + os.path.splitext(filename)[0] + '_flu_grained.json')\n",
    "        df_name = os.path.splitext(filename)[0] \n",
    "        df = pd.DataFrame(result)\n",
    "        df.loc[:, 'prob_score'] = df['all_responses'].apply(\n",
    "            lambda x: np.mean([int(i[-1]) for i in x if i and i[-1].isdigit()]))\n",
    "        df = [df['all_responses'][0], df['prob_score'][0]]\n",
    "        globals()[df_name + '_gpt_flu_grained'] = df\n",
    "        gpt_flu_grained[df_name + '_gpt_flu_grained'] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_out_2_gpt_flu_grained : 4.2\n",
      "frat_boy_genius_gpt_flu_grained : 4.7\n",
      "king_richard_gpt_flu_grained : 5.0\n",
      "frat_boy_genius_2_gpt_flu_grained : 4.9\n",
      "inception_gpt_flu_grained : 4.8\n",
      "dunkirk_2_gpt_flu_grained : 4.7\n",
      "dunkirk_gpt_flu_grained : 4.7\n",
      "king_richard_2_gpt_flu_grained : 5.0\n",
      "get_out_gpt_flu_grained : 4.7\n",
      "inception_2_gpt_flu_grained : 4.8\n"
     ]
    }
   ],
   "source": [
    "for name, value in gpt_flu_grained.items():\n",
    "    print(f\"{name} : {value[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CHARACTERS\n",
    "\n",
    "directory = 'data'\n",
    "gpt_char = {}\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".json\"):\n",
    "        result = gpt_movie_analysis(\"prompts/specific/char_movie.txt\", os.path.join(directory, filename), \"gpt-4-turbo-2024-04-09\", n=10, save='results/movies/gpt/' + os.path.splitext(filename)[0] + '_char.json')\n",
    "        df_name = os.path.splitext(filename)[0] \n",
    "        df = pd.DataFrame(result)\n",
    "        df.loc[:, 'prob_score'] = df['all_responses'].apply(\n",
    "            lambda x: np.mean([int(i[-1]) for i in x if i and i[-1].isdigit()]))\n",
    "        df = [df['all_responses'][0], df['prob_score'][0]]\n",
    "        globals()[df_name + '_gpt_char'] = df\n",
    "        gpt_char[df_name + '_gpt_char'] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_out_2_gpt_char : 3.5\n",
      "frat_boy_genius_gpt_char : 3.4\n",
      "king_richard_gpt_char : 4.9\n",
      "frat_boy_genius_2_gpt_char : 2.7\n",
      "inception_gpt_char : 4.5\n",
      "dunkirk_2_gpt_char : 4.0\n",
      "dunkirk_gpt_char : 4.4\n",
      "king_richard_2_gpt_char : 4.9\n",
      "get_out_gpt_char : 3.4\n",
      "inception_2_gpt_char : 4.7\n"
     ]
    }
   ],
   "source": [
    "for name, value in gpt_char.items():\n",
    "    print(f\"{name} : {value[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "### AI Judgment\n",
    "\n",
    "directory = 'data'\n",
    "gpt_jug = {}\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".json\"):\n",
    "        result = gpt_movie_analysis(\"prompts/specific/jug_movie.txt\", os.path.join(directory, filename), \"gpt-4-turbo-2024-04-09\", n=10, save='results/movies/gpt/' + os.path.splitext(filename)[0] + '_jug.json')\n",
    "        df_name = os.path.splitext(filename)[0] \n",
    "        df = pd.DataFrame(result)\n",
    "        df.loc[:, 'prob_score'] = df['all_responses'].apply(\n",
    "            lambda x: np.mean([int(i[-1]) for i in x if i and i[-1].isdigit()]))\n",
    "        df = [df['all_responses'][0], df['prob_score'][0]]\n",
    "        globals()[df_name + '_gpt_jug'] = df\n",
    "        gpt_jug[df_name + '_gpt_jug'] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_out_2_gpt_jug : 4.6\n",
      "frat_boy_genius_gpt_jug : 1.3\n",
      "king_richard_gpt_jug : 4.8\n",
      "frat_boy_genius_2_gpt_jug : 1.7\n",
      "inception_gpt_jug : 3.2\n",
      "dunkirk_2_gpt_jug : 4.2\n",
      "dunkirk_gpt_jug : 5.0\n",
      "king_richard_2_gpt_jug : 3.2\n",
      "get_out_gpt_jug : 2.0\n",
      "inception_2_gpt_jug : 4.4\n"
     ]
    }
   ],
   "source": [
    "for name, value in gpt_jug.items():\n",
    "    print(f\"{name} : {value[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clarity\n",
    "\n",
    "directory = 'data'\n",
    "gpt_cla = {}\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".json\"):\n",
    "        result = gpt_movie_analysis(\"prompts/specific/cla_movie.txt\", os.path.join(directory, filename), \"gpt-4-turbo-2024-04-09\", n=10, save='results/movies/gpt/' + os.path.splitext(filename)[0] + '_cla.json')\n",
    "        df_name = os.path.splitext(filename)[0] \n",
    "        df = pd.DataFrame(result)\n",
    "        df.loc[:, 'prob_score'] = df['all_responses'].apply(\n",
    "            lambda x: np.mean([int(i[-1]) for i in x if i and i[-1].isdigit()]))\n",
    "        df = [df['all_responses'][0], df['prob_score'][0]]\n",
    "        globals()[df_name + '_gpt_cla'] = df\n",
    "        gpt_cla[df_name + '_gpt_cla'] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_out_2_gpt_cla : 4.1\n",
      "frat_boy_genius_gpt_cla : 4.5\n",
      "king_richard_gpt_cla : 4.9\n",
      "frat_boy_genius_2_gpt_cla : 3.7\n",
      "inception_gpt_cla : 4.2\n",
      "dunkirk_2_gpt_cla : 3.6\n",
      "dunkirk_gpt_cla : 4.7\n",
      "king_richard_2_gpt_cla : 4.5\n",
      "get_out_gpt_cla : 4.1\n",
      "inception_2_gpt_cla : 4.3\n"
     ]
    }
   ],
   "source": [
    "for name, value in gpt_cla.items():\n",
    "    print(f\"{name} : {value[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/LLM_research/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/homebrew/anaconda3/envs/LLM_research/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "### Logline\n",
    "\n",
    "directory = 'data/logline'\n",
    "gpt_log = {}\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".json\"):\n",
    "        result = gpt_movie_analysis(\"prompts/specific/log_movie.txt\", os.path.join(directory, filename), \"gpt-4-turbo-2024-04-09\", n=10, save='results/movies/gpt/' + os.path.splitext(filename)[0] + '_log.json')\n",
    "        df_name = os.path.splitext(filename)[0] \n",
    "        df = pd.DataFrame(result)\n",
    "        df.loc[:, 'prob_score'] = df['all_responses'].apply(\n",
    "            lambda x: np.mean([int(i[-1]) for i in x if i and i[-1].isdigit()]))\n",
    "        df = [df['all_responses'][0], df['prob_score'][0]]\n",
    "        globals()[df_name + '_gpt_log'] = df\n",
    "        gpt_log[df_name + '_gpt_log'] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_out_log_2_gpt_log : 3.3\n",
      "frat_boy_log_2_gpt_log : 2.6\n",
      "king_richard_log_2_gpt_log : nan\n",
      "frat_boy_log_gpt_log : 2.3\n",
      "get_out_log_gpt_log : 3.0\n",
      "inception_log_gpt_log : 3.9\n",
      "dunkirk_log_2_gpt_log : 2.9\n",
      "inception_log_2_gpt_log : 3.8\n",
      "dunkirk_log_gpt_log : 3.6\n",
      "king_richard_log_gpt_log : 3.9\n"
     ]
    }
   ],
   "source": [
    "for name, value in gpt_log.items():\n",
    "    print(f\"{name} : {value[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
